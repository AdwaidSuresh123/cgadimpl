#include "ad/ops/nodeops.hpp"
#include <cuda_runtime.h>
#include "tensor.hpp" 
#include <unordered_map>
#include <cmath> 
#include <type_traits> 
#include "mlp/loss.h"

namespace ag {
namespace detail {
std::shared_ptr<Node> mse_loss_nodeops(const std::shared_ptr<Node>& pred, const std::shared_ptr<Node>& target) {
    Tensor diff = pred->value - target->value;
    Tensor sq   = diff * diff;
    Tensor loss = OwnTensor::reduce_mean(sq); 
    auto n = std::make_shared<Node>(loss, Op::MSELoss, (pred->requires_grad()), "mseloss");
    n->inputs = {pred, target};
    if (pred) pred->child_grad_count++;
    if (target) target->child_grad_count++;
    ag::debug::on_node_created(n);
    return n;
}
std::shared_ptr<Node> mae_loss_nodeops(const std::shared_ptr<Node>& pred, const std::shared_ptr<Node>& target) {
    Tensor diff = pred->value - target->value;
    Tensor abs_diff = OwnTensor::abs(diff, ag::current_stream());
    Tensor loss = OwnTensor::reduce_mean(abs_diff);
    auto n = std::make_shared<Node>(loss, Op::MAELoss, (pred->requires_grad() || target->requires_grad()), "maeloss");
    n->inputs = {pred, target};
    if (pred) pred->child_grad_count++;
    if (target) target->child_grad_count++;
    ag::debug::on_node_created(n);
    return n;
}
std::shared_ptr<Node> cross_entropy_with_logits_nodeops(const std::shared_ptr<Node>& logits, const std::shared_ptr<Node>& onehot){
    const Tensor& Z = logits->value;
    const Tensor& Y = onehot->value;
    Tensor max_val = OwnTensor::reduce_max(Z, {-1}, true);
    Tensor z_shifted = Z - max_val;
    Tensor log_sum_exp = OwnTensor::log(OwnTensor::reduce_sum(OwnTensor::exp(z_shifted, ag::current_stream()), {-1}, true), ag::current_stream());
    Tensor log_sm = z_shifted - log_sum_exp;
    Tensor prod = Y * log_sm;
    Tensor sum_prod = OwnTensor::reduce_sum(prod, {-1}); 
    Tensor loss = OwnTensor::reduce_mean(sum_prod * -1.0f); 
    auto n = std::make_shared<Node>(loss, Op::CeWithLogits, (logits->requires_grad() || onehot->requires_grad()), "ce_with_logits");
    n->inputs = {logits, onehot};
    if (logits) logits->child_grad_count++;
    if (onehot) onehot->child_grad_count++;
    ag::debug::on_node_created(n);
    return n;
}

std::shared_ptr<Node> kldivergence_nodeops(const std::shared_ptr<Node>& logits, const std::shared_ptr<Node>& onehot){
    const Tensor& Z = logits->value;
    const Tensor& Y = onehot->value;
    Tensor log_Y = OwnTensor::log(Y + 1e-9f, ag::current_stream());
    Tensor max_val = OwnTensor::reduce_max(Z, {-1}, true);
    Tensor z_shifted = Z - max_val;
    Tensor log_sum_exp = OwnTensor::log(OwnTensor::reduce_sum(OwnTensor::exp(z_shifted, ag::current_stream()), {-1}, true), ag::current_stream());
    Tensor log_sm_Z = z_shifted - log_sum_exp;
    Tensor kl_div_elementwise = Y * (log_Y - log_sm_Z);
    Tensor sum_kl = OwnTensor::reduce_sum(kl_div_elementwise, {-1});
    Tensor loss = OwnTensor::reduce_mean(sum_kl);
    auto n = std::make_shared<Node>(loss, Op::KLDivergence, (logits->requires_grad() || onehot->requires_grad()), "kldivergence");
    n->inputs = {logits, onehot};
    if (logits) logits->child_grad_count++;
    if (onehot) onehot->child_grad_count++;
    ag::debug::on_node_created(n);
    return n;
}

std::shared_ptr<Node> binary_cross_entropy_nodeops(const std::shared_ptr<Node>& pred, const std::shared_ptr<Node>& target) {

    // Tensor diff = pred->value - target->value;
    // Tensor sq   = diff * diff;
    // // --- THIS IS THE BUG ---
    // // It should be reduce_mean, not reduce_sum. `reduce_mean` correctly
    // // computes the VJP for the mean operation. `sum` has a different VJP.
    // Tensor loss = OwnTensor::reduce_mean(sq); 
    Tensor loss = OwnTensor::mlp_forward::binary_cross_entropy(pred->value, target->value);
    // --- END BUG ---

    auto n = std::make_shared<Node>(loss, Op::BinaryCrossEntropy, (pred->requires_grad() || target->requires_grad()), "binary_cross_entropy");
    n->inputs = {pred, target};

    if (pred) pred->child_grad_count++;
    if (target) target->child_grad_count++;

    ag::debug::on_node_created(n);
    return n;
}

std::shared_ptr<Node> categorical_cross_entropy_nodeops(const std::shared_ptr<Node>& pred, const std::shared_ptr<Node>& target) {

    // Tensor diff = pred->value - target->value;
    // Tensor sq   = diff * diff;
    // // --- THIS IS THE BUG ---
    // // It should be reduce_mean, not reduce_sum. `reduce_mean` correctly
    // // computes the VJP for the mean operation. `sum` has a different VJP.
    // Tensor loss = OwnTensor::reduce_mean(sq); 
    Tensor loss = OwnTensor::mlp_forward::categorical_cross_entropy(pred->value, target->value);
    // --- END BUG ---

    auto n = std::make_shared<Node>(loss, Op::CategoricalCrossEntropy, (pred->requires_grad() || target->requires_grad()), "categorical_cross_entropy");
    n->inputs = {pred, target};

    if (pred) pred->child_grad_count++;
    if (target) target->child_grad_count++;

    ag::debug::on_node_created(n);
    return n;
}

std::shared_ptr<Node> sparse_cross_entropy_with_logits_nodeops(const std::shared_ptr<Node>& logits, const std::shared_ptr<Node>& target){  //vis
    const Tensor& Z = logits->value;
    const Tensor& Y = target->value;
    Tensor max_val = OwnTensor::reduce_max(Z, {-1}, true);
    Tensor z_shifted = Z - max_val;
    Tensor log_sum_exp = OwnTensor::log(OwnTensor::reduce_sum(OwnTensor::exp(z_shifted, ag::current_stream()), {-1}, true), ag::current_stream());
    Tensor log_sm_Z = z_shifted - log_sum_exp;
    
    // Use the new gather operation (works on both CPU and GPU)
    Tensor selected_log_probs = OwnTensor::gather(log_sm_Z, 1, Y);

    Tensor loss = OwnTensor::reduce_mean(selected_log_probs * -1.0f); 
    auto n = std::make_shared<Node>(loss, Op::SparseCeWithLogits, (logits->requires_grad() || target->requires_grad()), "sparse_ce_with_logits");
    n->inputs = {logits, target};
    if (logits) logits->child_grad_count++;
    if (target) target->child_grad_count++;
    ag::debug::on_node_created(n);
    return n;
}
} // namespace detail
} // namespace ag